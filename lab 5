#demonstration of early stopping with the help of a dense neural network
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.datasets import mnist
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
x_train = x_train.reshape((x_train.shape[0], 28*28))
x_test = x_test.reshape((x_test.shape[0], 28*28))
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)
model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(28*28,)))
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
early_stopping = EarlyStopping(monitor='val_loss',
                               patience=5,
                               restore_best_weights=True)
history = model.fit(x_train, y_train,
                    epochs=50,
                    batch_size=64,
                    validation_data=(x_test, y_test),
                    callbacks=[early_stopping])
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")



#dropout
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt


(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

x_train = x_train.reshape((-1, 28 * 28))
x_test = x_test.reshape((-1, 28 * 28))

def build_model_with_dropout(dropout_rate=0.2):
    model = models.Sequential()
    model.add(layers.Dense(128, activation='relu', input_shape=(28*28,)))
    model.add(layers.Dropout(dropout_rate))  
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dropout(dropout_rate)) 
    model.add(layers.Dense(10, activation='softmax'))  
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model


def build_model_without_dropout():
    model = models.Sequential()
    model.add(layers.Dense(128, activation='relu', input_shape=(28*28,)))
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dense(10, activation='softmax'))  
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model


model_with_dropout = build_model_with_dropout(dropout_rate=0.2)
history_with_dropout = model_with_dropout.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))


model_without_dropout = build_model_without_dropout()
history_without_dropout = model_without_dropout.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))


plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history_with_dropout.history['accuracy'], label='Train Accuracy')
plt.plot(history_with_dropout.history['val_accuracy'], label='Test Accuracy')
plt.title('Model with Dropout')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(history_without_dropout.history['accuracy'], label='Train Accuracy')
plt.plot(history_without_dropout.history['val_accuracy'], label='Test Accuracy')
plt.title('Model without Dropout')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()
